{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Artificial Neural Networks\n",
    "\n",
    "---\n",
    "### ## Biological Neurons\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65779651-5cbe2f80-e172-11e9-948d-b3dc61e00c9d.png\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65779749-8d9e6480-e172-11e9-8e08-4c69ea0d052b.png\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65779891-ce967900-e172-11e9-8ee9-6840fc3a6780.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Neurons\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65780010-07cee900-e173-11e9-8b10-9fb380a9ed74.png\" style=\"width:500px\"/>\n",
    "\n",
    "###### \"Brain\" analogies\n",
    "* Be very careful with your analogies\n",
    "* It is inspired by how the brain works, \u000b",
    "but do not say that it works like a brain\n",
    "\n",
    "### ## Biological vs Artificial\n",
    "* Complex connectivity Pattern\n",
    "* Many different types\n",
    "* Dendrites can perform complex \u000b",
    "nonlinear computations\n",
    "* Synapses are not a single weight \u000b",
    "but a complex non-linear dynamical system\n",
    "* Rate code may not be adequate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Artificial Neural Networks History\n",
    "\n",
    "---\n",
    "* Frank Rosenblatt, ~1957: Mark I Perceptron\n",
    "    * the first implementation of perceptron algorithm\n",
    "    * The machine was connected to a camera that used 20×20 cadmium sulfide photocells to produce a 400-pixel image. \n",
    "    * Recognized letters of the alphabet\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65780726-8d06cd80-e174-11e9-8378-9a2b063d4da4.png\" style=\"width:500px\"/>\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65780826-c0495c80-e174-11e9-9748-7933df421b6e.png\" style=\"width:230px\"/>\n",
    "\n",
    "* Widrow and Hoff, ~1960: Adaline/Madaline\n",
    "    * First multilayer perceptron network\n",
    "    * Stacked perceptron machine\n",
    "    * Still no Backpropagation\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65781365-b3793880-e175-11e9-8b30-bbbca5890227.png\" style=\"width:380px\"/>\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65781389-c1c75480-e175-11e9-9e97-fa3077519e8f.png\" style=\"width:230px\"/>\n",
    "\n",
    "* Rumelhart et al. 1986: \n",
    "    * First time back-propagation became popular\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65781587-123eb200-e176-11e9-9c7e-eb5befc390d3.png\" style=\"width:230px\"/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Problem in Neural Network\n",
    "\n",
    "---\n",
    "### ## Black Box Magic\n",
    "\n",
    "* Magically happen to performed well\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65781747-634ea600-e176-11e9-80d0-cebdd5d4c44c.png\" style=\"width:300px\"/>\n",
    "* It is difficult to know why a Neural Net made a particular decision, \n",
    "* or which data features where most important in making that decision\n",
    "* we know that linear classifier is like creating image template for each class\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782034-01427080-e177-11e9-9231-e28b94e2f029.png\" style=\"width:300px\"/>\n",
    "* And each neuron in Multi Layered Perceptron is a linear classifier doing some classification\n",
    "* That’s why Neurons in Hidden layer are always described as sub-classifier which recognize features\n",
    "* Then the next neuron combines small features into larger features and continue further until the class classification\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782136-38188680-e177-11e9-9137-fb5d85fca592.png\" style=\"width:300px\"/>\n",
    "* The deeper the network, the more and more complex features are formed before reaching a decision\n",
    "* This description cannot be proven as only the first layer in Neural Network can be visualized\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782260-7c0b8b80-e177-11e9-8531-16edf2276cee.png\" style=\"width:300px\"/>\n",
    "\n",
    "### ## Data Availability\n",
    "* The theory: with more data to learn, ANN can be trained further and became \"smarter\"\n",
    "* The problem?\n",
    "    * Scarce data to train\n",
    "    * Not enough storage\n",
    "    \n",
    "### ## back propagation Problem\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782474-f20ff280-e177-11e9-89fb-0179f2254ab5.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Vanishing Gradient Problem\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782703-76627580-e178-11e9-9390-ec3890320f1a.png\" style=\"width:500px\"/>\n",
    "\n",
    "> Remember, Gradient is a feedback value that must be added to the weights so that loss can be reduced\n",
    "\n",
    "* The front layers can't learn (weights not updated)\n",
    "* It is said that the effective standard ANN is around 4 layers\n",
    "\n",
    "### ## Serious Problem in Deep Neural Networks\n",
    "* ConvNet: Millions of parameters\n",
    "* Gradient vanished in an instant\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65782995-0ef8f580-e179-11e9-8421-1188c7f32dda.png\" style=\"width:500px\"/>\n",
    "* Neural Turing Machine\n",
    "    * Recurrent Net\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65783112-4cf61980-e179-11e9-884a-894b842793e2.png\" style=\"width:300px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Deep Learning Arrival\n",
    "\n",
    "---\n",
    "### ## Layer-wise PreTraining\n",
    "* Do not train all layers in the network at once\n",
    "* Instead, train each layer one-by-one in unsupervised manner to build the feature extraction ability in stages\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65783350-d4dc2380-e179-11e9-882b-12f0140cc4f8.png\" style=\"width:300px\"/>\n",
    "* After all layers have been trained, \u000b",
    "merge the layers back into one classification network\n",
    "* First time backpropagation train \u000b",
    "properly in more than 10 layers \u000b",
    "with significant results\n",
    "* Hence the name:\n",
    "\t**DEEP LEARNING**\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65783516-27b5db00-e17a-11e9-8a8a-40559df576a9.png\" style=\"width:130px\"/>\n",
    "\n",
    "### ## Deep Belief Network\n",
    "\n",
    "* Introduced by Geoffrey E Hinton and Russ Salakhutdinov in 2006\n",
    "* Using stacked Restricted Boltzmann Machine \n",
    "* The word Deep from DBN began to be popularized as Deep Learning\n",
    "* Reimagined and reinvigorated research in AI/ML\n",
    "* Most CS research came back to the area\n",
    "* Popularizing Deep Learning brand\n",
    "\n",
    "\n",
    "### ## Dawn of ConvNet\n",
    "* Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, 2012\n",
    "    * Imagenet classification with deep convolutional neural networks\n",
    "    * Geoff Hinton began to be referred to by some as the \"Godfather of Deep Learning“\n",
    "    \n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65846089-db7bbe00-e366-11e9-835b-ee3b352fdc33.png\" style=\"width:400px\"/>\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65846127-fa7a5000-e366-11e9-87e4-447a0508cc64.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Aside: Other Deep Learning\n",
    "\n",
    "---\n",
    "* Has been developed long before the term was actually introduced\n",
    "    * GMDH Polynomial Neural Networks\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65846277-6eb4f380-e367-11e9-9678-f915686e37b6.png\" style=\"width:350px\"/>\n",
    "    * Nonlinear PCA has an architecture that\u000b",
    "similar to Auto Encoder\n",
    "    * Used to be called \u000b",
    "Bottleneck-Autoassociative Neural Network Architecture\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65846383-c18eab00-e367-11e9-8879-d86413c9039d.png\" style=\"width:350px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Aside: Auto Encoder\n",
    "\n",
    "---\n",
    "* consist of 2 Network side-by-side called Encode and Decoder\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65847680-ba1dd080-e36c-11e9-907c-98d4f5369a4c.png\" style=\"width:350px\"/>\n",
    "\n",
    "* Encoder network accepts inputs and try to reduce it into a smaller dimension\n",
    "* Decoder network receives reduced dimension from Encoder and tries to restore it to its original dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Aside: Layer-wise Pretraining\n",
    "\n",
    "---\n",
    "* Pretraining Layer 1\n",
    "    * Train Encoder to reduce the dimension\n",
    "    * In a way that it can still be reconstructed back by decoder\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848120-ca36af80-e36e-11e9-89c0-a9985bcc8ed7.png\" style=\"width:350px\"/>\n",
    "    * Trained using MSE loss against the input\n",
    "    * After it converges, use the encoder to reduce all data\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848203-3d402600-e36f-11e9-95c3-e72c86b911d4.png\" style=\"width:350px\"/>\n",
    "\n",
    "* Pretraining Layer 2\n",
    "    * Repeat process and use the reduced data to train \u000b",
    "second encoder to reduce the dimension even further\n",
    "    * Repeat for all designed layers\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848255-6fea1e80-e36f-11e9-913f-40ba8290ed52.png\" style=\"width:350px\"/>\n",
    "\n",
    "* Train Supervised\n",
    "    * Stacks all trained encoder layers\n",
    "    * Finetune (backprop) the network to classify target\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848362-ee46c080-e36f-11e9-8723-5c36d6ab4a12.png\" style=\"width:350px\"/>\n",
    "    * Converge much better and faster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Activation Functions\n",
    "\n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848431-467dc280-e370-11e9-984f-0cb5ff1392bd.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Sigmoid Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848538-be4bed00-e370-11e9-8bb4-b0f5b66dda2a.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65848569-df144280-e370-11e9-9717-6937866d2d3d.png\" style=\"width:200px\"/>\n",
    "* Backward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65849715-eab63800-e375-11e9-8553-fb3a99482b7b.png\" style=\"width:200px\"/>\n",
    "* Squashes numbers to range [0, 1]\n",
    "* Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron\n",
    "\n",
    "#### ### Sigmoid Function Problems\n",
    "* Saturated neurons “kill” the gradients during backward pass\n",
    "    * Vanishing gradient problem\n",
    "    * Example when x ≥10 or ≤-10\n",
    "* Sigmoid outputs are not zero-centered\n",
    "    * All positive/all negative gradient if input is not zero-centered\n",
    "    * Inefficient update\n",
    "* Expensive computation\n",
    "    * Calculating exponent\n",
    "    \n",
    "### ## Tanh Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65849871-8c3d8980-e376-11e9-902a-1a2e050a87b6.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65849911-abd4b200-e376-11e9-97e2-95ebd29b58dd.png\" style=\"width:200px\"/>\n",
    "* Backward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65849920-b727dd80-e376-11e9-839d-c3b17b05a9e7.png\" style=\"width:200px\"/>\n",
    "* Squashes numbers to range [-1, 1]\n",
    "* zero-centered (nice)\n",
    "* still kills gradients when saturated\n",
    "\n",
    "### ## Refined Linear Unit\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850228-3e298580-e378-11e9-9d25-bbc79d863e4f.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850246-50a3bf00-e378-11e9-96cb-87060e784308.png\" style=\"width:200px\"/>\n",
    "* Backward function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850284-74ff9b80-e378-11e9-9e65-6e30ce34eb9a.png\" style=\"width:200px\"/>\n",
    "* Does not saturate (in +region)\n",
    "* Very computationally efficient\n",
    "* Converges much faster than sigmoid/tanh in practice\n",
    "* Current Default Recommendation\n",
    "\n",
    "#### ### Refined Linear Unit Problems\n",
    "* Current Default Recommendation\n",
    "* Not zero-centered output\n",
    "* Kills neuron with zero-output\n",
    "    * Not just squashed it, but kills it completely \n",
    "    * Dead ReLU\n",
    "* 10-20% of ReLU neurons are usually dead\n",
    "    * Dead ReLU will never activate, never updated\n",
    "* Possible cause:\n",
    "    * Unlucky when weight initialization\n",
    "    * Too high Learning Rate\n",
    "    * Attempts to fix: initialize slightly positive bias\n",
    "    \n",
    "### ## Leaky ReLU\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850469-3cac8d00-e379-11e9-858c-049d6ad9d660.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850520-72517600-e379-11e9-80ad-cbbbd71791bd.png\" style=\"width:200px\"/>\n",
    "* backward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850526-78475700-e379-11e9-9715-0c3f3eab8db3.png\" style=\"width:200px\"/>\n",
    "Try to fix “dead” Neuron in ReLU\n",
    "Does not saturate; Computationally efficient\n",
    "Converges much faster than sigmoid/tanh in practice (~6x)\n",
    "Will not “die”\n",
    "\n",
    "    \n",
    "### ## Prametic ReLU\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850646-e25ffc00-e379-11e9-8651-bf4127bf6678.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850660-ef7ceb00-e379-11e9-93e5-e1e682235971.png\" style=\"width:200px\"/>\n",
    "* backward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850982-5f3fa580-e37b-11e9-8a9e-d5c0cc69dfa4.png\" style=\"width:200px\"/>\n",
    "* Parameter alpha is learnable through backpropagation\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65850992-6e265800-e37b-11e9-9cd4-9c7324570bf6.png\" style=\"width:200px\"/>\n",
    "\n",
    "### ## Exponential Linear Units\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851052-a9c12200-e37b-11e9-976c-b31094280f2b.png\" style=\"width:200px\"/>\n",
    "\n",
    "* Forward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851074-ba719800-e37b-11e9-8755-a40789dee5cf.png\" style=\"width:200px\"/>\n",
    "* backward Function\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851136-fa387f80-e37b-11e9-96f2-59bbd6b919f9.png\" style=\"width:200px\"/>\n",
    "* All benefits of ReLU\n",
    "* Does not “die”\n",
    "* Does not saturate, \n",
    "* Does not die\n",
    "* Converges much faster\n",
    "* Closer to zero mean outputs; \n",
    "* Negative saturation, adds some robustness to noise\n",
    "* Problem: Computation requires exp()\n",
    "\n",
    "### ## More of ReLU\n",
    "* Scaled exponential linear unit (SELU)\n",
    "* Randomized leaky ReLU (RReLU)\n",
    "* S-shaped ReLU (SReLU)\n",
    "* Adaptive piecewise linear (APL)\n",
    "\n",
    "### ## why use Activation Function?\n",
    "Without activation function, the neural network will be just a single linear sandwich\n",
    "The capacity is the same as just a linear classifier\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851345-ca3dac00-e37c-11e9-9c9c-f4772df2fdf8.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Weight Initialization\n",
    "\n",
    "---\n",
    "### ## Weight Initialization Observation\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851421-125cce80-e37d-11e9-9975-c6ab59d5369d.png\" style=\"width:300px\"/>\n",
    "\n",
    "* Train several layers of Neural Network\n",
    "* Input 1000 parameters\n",
    "* 10 hidden layers \n",
    "* @500 neurons\n",
    "* Using tanh activation\n",
    "* Forward propagate once using single data\n",
    "* Record output activation statistic in each layer\n",
    "* Calculate mean and std\n",
    "* Show plot and histogram\n",
    "\n",
    "### ## Zeros and Ones\n",
    "* What happens if we initialized 𝑊=𝑧𝑒𝑟𝑜𝑠?\u000b",
    "Or if we initialized 𝑊=𝑜𝑛𝑒𝑠 or 𝑊=𝐶? \n",
    "    * All neurons will compute the same thing\n",
    "    * Backprop behave the same way\n",
    "    * No Symmetry breaking\n",
    "    \n",
    "### ## Small Random Initialization\n",
    "* What if we initialized with small random numbers\n",
    "    * W = 0,01 * np.random.randn(dim,hid)\n",
    "    * Gaussian with zero mean and std 1e-2 \n",
    "* Works okay for small networks\n",
    "* but can lead to non-homogeneous distributions of activations across the layers of a deep network\n",
    "* Forward propagate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851589-a29b1380-e37d-11e9-86c0-248666e5d89e.png\" style=\"width:500px\"/>\n",
    "* **Too small. all activations become zero**\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851658-edb52680-e37d-11e9-8b6b-889089784889.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Random Normal Initialization\n",
    "* Just random numbers\n",
    "    * W = np.random.randn(dim,hid)\n",
    "    * Gaussian with zero mean and std 1\n",
    "* Forward propagate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851731-235a0f80-e37e-11e9-8b73-3b57600adedd.png\" style=\"width:500px\"/>\n",
    "* **Too big, all activations saturated**\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851782-53a1ae00-e37e-11e9-95e0-5e48cc6aafad.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Xavier Initialization\n",
    "* Scale number based on input dimension\n",
    "    * W = np.random.randn(dim, hid) / np.sqrt(dim)\n",
    "    * [Glorot et al., 2010] \n",
    "* Forward propagate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851842-8186f280-e37e-11e9-9616-f5af49cc2d92.png\" style=\"width:500px\"/>\n",
    "* Reasonable Initialization\n",
    "* Nothing is super saturated\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65851902-aa0eec80-e37e-11e9-8d26-8dedfbdb0237.png\" style=\"width:500px\"/>\n",
    "\n",
    "* **Breaks when using ReLU**\n",
    "* Forward Propagate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852045-27d2f800-e37f-11e9-8560-132606c1d2c7.png\" style=\"width:500px\"/>\n",
    "* Activations back to zero\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852089-4a651100-e37f-11e9-8a9c-88be927648cc.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## kaiming He Initialization\n",
    "* Half the distribution for ReLU\n",
    "    * W = np.random.randn(dim, hid) / np.sqrt(dim / 2)\n",
    "    * [He et al., 2015] \n",
    "* Forward propagate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852173-8d26e900-e37f-11e9-86ac-25b529c025cf.png\" style=\"width:500px\"/>\n",
    "* Works on ReLU\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852243-c52e2c00-e37f-11e9-886a-5982887573f7.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Batch Normalization\n",
    "\n",
    "---\n",
    "### ## Problem Internal Covarienace Shift\n",
    "* Change of distribution in activation across layers\n",
    "* Sensitive to saturations\n",
    "* Need really small Learning Rate\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852366-1d652e00-e380-11e9-9717-d21b73d8fe44.png\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Batch Normalization\n",
    "* A technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance \n",
    "* You can keep using Gaussian activation\n",
    "* Consider a batch of activation at some layer\n",
    "    * Take a mini batch through the network\n",
    "    * Normalize each input dimension\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65852754-5651d280-e381-11e9-8be2-ba2ab1c5856a.png\" style=\"width:200px\"/>\n",
    "* Differentiable function\n",
    "    * Use it though training pipeline\n",
    "    \n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65853135-ae3d0900-e382-11e9-9103-8068288b0019.png\" style=\"width:650px\"/>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65853203-e9d7d300-e382-11e9-8cb1-52a14e3f4f62.png\" style=\"width:380px\"/>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65853296-37544000-e383-11e9-8355-3a7d6d521248.png\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "* Improves gradient flow through the network\n",
    "* Allows higher learning rates\n",
    "    * Network can learn faster\n",
    "* Reduces the strong dependence on initialization\n",
    "    * Huge difference\n",
    "    * Works with much larger initialization scale\n",
    "* Acts as a form of regularization (in a funny way)\n",
    "* Slightly reduces the need for dropout\n",
    "\n",
    "### ## Batch Normalization Problems\n",
    "\n",
    "* 𝛽 and 𝛾 add complexity and are largely irrelevant to the issues under study. \n",
    "* Learnable 𝛽 are recommended whilst learnable scales 𝛾 are sometimes actively unhelpful\n",
    "* it’s slow (although node fusion can help)\n",
    "* it’s different at training and test time and therefore fragile\n",
    "* it’s ineffective for small batches and various layer types\n",
    "* it has multiple interacting effects which are hard to separate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Batch Normalization Back Propagation\n",
    "\n",
    "---\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65853553-16d8b580-e384-11e9-9607-1f9de6b908d7.png\" style=\"width:500px\"/>\n",
    "\n",
    "* Using\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65862133-c53a2600-e397-11e9-9a08-f80fbcdb143f.png\" style=\"width:400px\"/>\n",
    "* Backward pass\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65853723-a5e5cd80-e384-11e9-8a6e-2cef89da8406.png\" style=\"width:400px\"/>\n",
    "\n",
    "### ## Test Time\n",
    "* Different BatchNorm function at Test Time\n",
    "    * The mean/std are not computed based on the batch. \n",
    "    * Instead, a single fixed empirical mean of activations during training is used.\n",
    "    * Use deterministic function\n",
    "    * Remember 𝝁 and 𝝈 while training across the dataset\n",
    "        * can be estimated during training with running averages\n",
    "* Calculate moving average and moving variance while training\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65862916-62498e80-e399-11e9-905f-bdc947b47c35.png\" style=\"width:300px\"/>\n",
    "* At test time\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65863369-2d8a0700-e39a-11e9-8fd9-7a3a0a73871b.png\" style=\"width:300px\"/>\n",
    "* In comparison, when Training\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65863436-50b4b680-e39a-11e9-92a3-a5598f0932fa.png\" style=\"width:500px\"/>\n",
    "* TL;DR What we need to keep during forward pass\n",
    "<img src=\"https://user-images.githubusercontent.com/38347258/65863526-7c37a100-e39a-11e9-9ac0-88009fdf60a8.png\" style=\"width:500px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
