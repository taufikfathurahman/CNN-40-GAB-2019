{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Convolutional Neural Network\n",
    "**(without the brain stuff)**\n",
    "\n",
    "---\n",
    "### ## Layer in CNN\n",
    "* 3 Main Layers\n",
    "    - Convolutional layer\n",
    "    - Pooling layer\n",
    "    - Fully COnnected\n",
    "* Common setting to use ReLU activation\n",
    "    * after each Convolution Layer\n",
    "    * after each FC Layer except the output Layer\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65979231-23056580-e49f-11e9-8ea7-f81cc0f52a01.png\" width=\"500px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Convolutional Layer\n",
    "---\n",
    "\n",
    "### ## Regular NN and Convolutinal NN\n",
    "* Regular Neural Net (affine layer) transforms (maps) an input vector \u000b",
    "(1-dimensional input) into an output vector (1-dimensional output)\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65979539-ad4dc980-e49f-11e9-9039-6744843deab8.png\" width=\"300px\"/></p>\n",
    "* ConvNet Layers transform a 3-dimensional input into another 3-dimensional output\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65979547-af178d00-e49f-11e9-8e09-755c323e9328.png\" width=\"300px\"/></p>\n",
    "\n",
    "### ## Fully Connected Layer\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65979752-09185280-e4a0-11e9-880c-cb503f98a0bf.png\" width=\"200px\"/></p>\n",
    "\n",
    "* The input area that is connected to a neuron, is called the **Neuron's Receptive Field**\n",
    "* At the FC layer, each neuron is connected to all inputs\n",
    "* Global Connectivity\n",
    "* While on image input, the relationship between pixels (spatial correlation) is local\n",
    "\n",
    "### ## Convolutional Layer\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65979924-63b1ae80-e4a0-11e9-94f5-e5097a4e40e9.png\" width=\"250px\"/></p>\n",
    "\n",
    "* Now the receptive field of a neuron is local\n",
    "* This template is called Filter or Kernel\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65980160-cacf6300-e4a0-11e9-8ad4-435bf977413c.png\" width=\"250px\"/></p>\n",
    "\n",
    "**Neuron activation:**\n",
    "> the result of taking a dot product between the filter and a small chunk of the image\n",
    "(i.e. 5*5*3 = 75-dimensional dot product + bias)\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65980290-0d913b00-e4a1-11e9-89a6-7d00a60e8d8d.png\" width=\"400px\"/></p>\n",
    "\n",
    "**Shared Weights:**\n",
    "> Each output in the activation map is calculated from the same weight matrix\n",
    "\n",
    "#### <> Ilustration\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65980679-d2dbd280-e4a1-11e9-9fca-315a06389041.png\" width=\"400px\"/></p>\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65981483-6b268700-e4a3-11e9-965e-61cf7f51ee7b.png\" width=\"400px\"/></p>\n",
    "\n",
    "* A filter creates a small feature template,\n",
    "* Activation value indicates the presence of that feature in that particular location\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65981617-b0e34f80-e4a3-11e9-8927-4536bea32892.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65981707-df612a80-e4a3-11e9-9096-5d1e35755d2c.png\" width=\"400px\"/></p>\n",
    "\n",
    "**Compute independently**\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983437-adea5e00-e4a7-11e9-8284-c53a1c0b0b0e.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983488-c9556900-e4a7-11e9-97a1-6449eed6823b.png\" width=\"400px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Kernel and Sizes\n",
    "\n",
    "---\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983558-f7d34400-e4a7-11e9-9e39-b7954b6b91b5.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983581-09b4e700-e4a8-11e9-8e5e-fad50674952b.png\" width=\"400px\"/></p>\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983637-349f3b00-e4a8-11e9-8c9d-2df633226b68.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983651-38cb5880-e4a8-11e9-91be-e3d1ad76663b.png\" width=\"400px\"/></p>\n",
    "\n",
    "---\n",
    "\n",
    "### ## The use of Padding\n",
    "\n",
    "> Preserve size spatially.\n",
    "\n",
    "> Shrinking size too fast is not good, in practice doesn’t work well\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983718-644e4300-e4a8-11e9-8822-d8d032a80124.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983723-6912f700-e4a8-11e9-82b0-ec0019923668.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984057-1a199180-e4a9-11e9-9434-0b2d47076802.png\" width=\"400px\"/></p>\n",
    "\n",
    "---\n",
    "### ## Convolutinal Layer Formula\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65983801-93fd4b00-e4a8-11e9-8972-1c3503a3ed62.png\" width=\"500px\"/></p>\n",
    "\n",
    "### ## Common Setting\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984161-5947e280-e4a9-11e9-99e1-a29e2f54a9f2.png\" width=\"500px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Convolutional Neural Network\n",
    "**(Brain/Neuron point of view)**\n",
    "\n",
    "---\n",
    "\n",
    "### # Convolutional Layer\n",
    "\n",
    "> ConvNet is a sequence of Convolutional Layers, interspersed with\n",
    "activation functions\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984666-7204c800-e4aa-11e9-8ac0-619a80f090c2.png\" width=\"400px\"/></p>\n",
    "\n",
    "### ## Feature Visualization of ConvNet\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984723-8f399680-e4aa-11e9-8a6e-5064a95d3313.png\" width=\"400px\"/></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984765-ad9f9200-e4aa-11e9-934a-ec785af543b2.png\" width=\"400px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Pooling Layer\n",
    "---\n",
    "\n",
    "### ## Pooling Layer - Motivation\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65984882-e8092f00-e4aa-11e9-87b0-69163aa0751f.png\" width=\"180px\"/></p>\n",
    "\n",
    "* Let’s say we have a filter which is an \"eye\" detector\n",
    "* Positive activation indicates the presence of \"eye\" \n",
    "* The filter is activated the moment an eye image enters the receptive area\n",
    "* How to make the detection robust to the exact location of the eye?\n",
    "* Pooling Layer filters and collects responses in neighboring locations\n",
    "* Obtain general area of ​​neuron activity\n",
    "* Gain robustness to the exact spatial location of features\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985057-4504e500-e4ab-11e9-90d7-84fbeb13f596.png\" width=\"300px\"/></p>\n",
    "\n",
    "* Effectively, the job of the pooling layer is to shrink the size of the activation map\n",
    "* Performs downsampling on spatial size, but not depth\n",
    "* Each activation map is down-sampled separately\n",
    "\n",
    "### ## Conv vs Pool\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985174-7d0c2800-e4ab-11e9-8d51-92293d7a41be.png\" width=\"500px\"/></p>\n",
    "\n",
    "* Preserve the spatial size when performing convolution, but increase (or decrease) the depth of the activation\n",
    "* Shrink the spatial size when pooling, but maintain the activation depth (number of features)\n",
    "\n",
    "### ## Max and Average Pooling\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985265-a88f1280-e4ab-11e9-80b9-5cc4b11f9cd0.png\" width=\"500px\"/></p>\n",
    "\n",
    "### ## Pooling Layer Formula\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985323-bd6ba600-e4ab-11e9-8fd3-915fc3883261.png\" width=\"500px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Fully Connected Layer\n",
    "---\n",
    "\n",
    "* The last output volume activation map is a collection and summary of activation features\n",
    "* In general, for classification, the output map will be Flatten then feed it into FC Layers for classification\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985650-6adeb980-e4ac-11e9-8175-94d496ad7dca.png\" width=\"400px\"/></p>\n",
    "\n",
    "* You can see that Convolution and Pooling layers act as a big feature extraction that feeds the classic Neural Net as a classifier\n",
    "* End-to-end Learning\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65985828-c6a94280-e4ac-11e9-86d9-067c5b03dff4.png\" width=\"400px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Backpropagation in Convolutional Neural Network\n",
    "---\n",
    "\n",
    "### ## Cross-Corelation vs Convolutional\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65986483-248a5a00-e4ae-11e9-9b70-415df14f9e3a.png\" width=\"500px\"/></p>\n",
    "\n",
    "* Convolution is the same as Cross-Correlation, \u000b",
    "except that the filter is \"flipped\" \n",
    "    * Horizontally and vertically\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65986597-5bf90680-e4ae-11e9-95b8-2810b94ff0ea.png\" width=\"150px\"/></p>\n",
    "\n",
    "* Many machine learning libraries implement Cross-Correlation but call it Convolution\n",
    "\n",
    "**Cross-Correlation and Convolutional is a kind of the same process**\n",
    "> Forward Convolution, backward Cross-correlation\n",
    "\n",
    "> Forward Cross-correlation, backward Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Forward Cross-Correlation\n",
    "\n",
    "---\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65987112-636cdf80-e4af-11e9-907c-859476688475.png\" width=\"400px\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Forward Convolution\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65987591-54d2f800-e4b0-11e9-82b6-6a6979b98e2d.png\" width=\"400px\"/></p>\n",
    "\n",
    "### ## Forward-Backward\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65987824-cc088c00-e4b0-11e9-94e6-f014c84dec96.png\" width=\"500px\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Backward Convolution\n",
    "\n",
    "---\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65988052-4c2ef180-e4b1-11e9-8b5e-d3be6d56a43f.png\" width=\"500px\"/></p>\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65988129-784a7280-e4b1-11e9-9421-fa3ab3c3291c.png\" width=\"500px\"/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Backward Pooling\n",
    "---\n",
    "\n",
    "* Max Pool\n",
    "    * Gradient only flows back to the max neuron (where it came from)\n",
    "    * Anywhere else = 0 (did not contribute)\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65988318-cb242a00-e4b1-11e9-9c07-eee6d2b9a51b.png\" width=\"300px\"/></p>\n",
    "\n",
    "* Average pool\n",
    "    * Gradient multiplied by 1/𝐹𝑥𝐹\n",
    "    * Assign to all neuron\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/65988318-cb242a00-e4b1-11e9-9c07-eee6d2b9a51b.png\" width=\"300px\"/></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
