{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient-based Learning\n",
    "\n",
    "---\n",
    "* Computing the Slope of the loss \u000b",
    "if we move the weights to a new location.\n",
    "<img src=\"assets/pic1.PNG\" style=\"width:400px\"/>\n",
    "* Move the weights to follow the greatest slope\n",
    "\n",
    "###### Computing the Gradient\n",
    "* Numerical Gradient\n",
    "    * using Iteriation\n",
    "* Analytic Gradient\n",
    "    * Using Calsulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ## Nemerical Gradient\n",
    "---\n",
    "* For each dimension in W\n",
    "    - Move the value slightly\n",
    "    - Calculate and record the Gradient (loss difference) of that particular dimension\n",
    "* Update W based on each dimensions gradient\n",
    "* Calculate the Gradient using\n",
    "<img src=\"assets/pic2.PNG\" style=\"width:300px\"/>\n",
    "* Or centered difference formula\n",
    "<img src=\"assets/pic3.PNG\" style=\"width:300px\"/>\n",
    "* Easy to wirite\n",
    "* Approximate (According to the step size *h*\n",
    "* Extremely slow on deep network\n",
    "    - Need to iterate throught all dimensions\n",
    "    - Very slow to calculate all\n",
    "<img src=\"assets/pic4.PNG\" style=\"width:700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ## Analyrical Gradient\n",
    "---\n",
    "* The loss is just a function of W\n",
    "* Gradient is just a generalization of the derative of a function\n",
    "<img src=\"assets/pic5.PNG\" style=\"width:500px\"/>\n",
    "* Derive of the gradients \n",
    "    - Flow back the gradient using back propagation\n",
    "* Exact, no approximation\n",
    "* Fast and Efficent\n",
    "* Error prone, because we have to do the math\n",
    "\n",
    "###### Gradient Check\n",
    "* In practice use Analytic Gradient\n",
    "* But always Check the implementation with Numerical Gradient\n",
    "* Result of Analytic Gradient must be ‚âà result of Numerical Gradient\n",
    "    *Then we can say that the model pass the Gradient Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Neural Networks\n",
    "##### (Without the brain stuff)\n",
    "---\n",
    "### ## Neuron\n",
    "* The main component of Neural Network\n",
    "* Which is actually just a simple linear function (rename from logistic regression)\n",
    "<img src=\"assets/pic6.PNG\" style=\"width:200px\"/>\n",
    "* Neural Network is just a series (stacks) of neuron\n",
    "* Each connection between neuron is precedented by a non-linear function\n",
    "<img src=\"assets/pic7.PNG\" style=\"width:300px\"/>\n",
    "* Before, if one neuron was able to make a line to divide class 1 and 0,\n",
    "<img src=\"assets/pic8.PNG\" style=\"width:400px\"/>\n",
    "* Then two neurons can create two lines\n",
    "* And combine them back to a single classification\n",
    "<img src=\"assets/pic9.PNG\" style=\"width:400px\"/>\n",
    "* (Before) Linear Score Function\n",
    "    - Single Layer Perceptron\n",
    "<img src=\"assets/pic10.PNG\" style=\"width:60px\"/>\n",
    "<img src=\"assets/pic12.PNG\" style=\"width:200px\"/>\n",
    "* (Now) 2-Layer Neural Network\n",
    "    - 1 Hidden Layer Neural Net\n",
    "<img src=\"assets/pic11.PNG\" style=\"width:100px\"/>\n",
    "<img src=\"assets/pic13.PNG\" style=\"width:300px\"/>\n",
    "* (and further) 3-Layer Neural Network\n",
    "    - 2 Hidden Layer Neural Net\n",
    "<img src=\"assets/pic15.PNG\" style=\"width:150px\"/>\n",
    "<img src=\"assets/pic14.PNG\" style=\"width:350px\"/>\n",
    "* Layer is where the weights attached\n",
    "* A network with twi or more layers is reffered to as Multi-Layer Perceptron (MLP)\n",
    "\n",
    "### ## Layer Number and Size\n",
    "* Increase the size and number == increase the network capacity\n",
    "    - Neural Networks with more neurons can express more complicated functions\n",
    "    - Can learn to classify more complicated data\n",
    "* Tradeoff:\n",
    "    - More likely to overfit the training data\n",
    "    \n",
    "<img src=\"assets/pic16.jpg\" style=\"width:600px\"/>\n",
    "\n",
    "> More neurons = more capacity = more likely to overfit data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient in a Neuron\n",
    "---\n",
    "### ## Neuron Structure\n",
    "<img src=\"assets/pic16.PNG\" style=\"width:250px\"/>\n",
    "\n",
    "> x1 = -3; w1 = 3\n",
    "\n",
    "> x2 = 4; w2 =2\n",
    "\n",
    "> f(x, w) = 1/(1+e^(-v))\n",
    "    - Sigmoid activation function\n",
    "    - Standard non-linearity\n",
    "\n",
    "> v = x1w1 + x2w2\n",
    "\n",
    "### ## Neuron Gate Structure\n",
    "<img src=\"assets/pic17.PNG\" style=\"width:250px\"/>\n",
    "\n",
    "> x1 = -3; w1 = 3\n",
    "\n",
    "> x2 = 4; w2 =2\n",
    "\n",
    "> f(x, w) = 1/(1+e^(-v))\n",
    "    - Sigmoid activation function\n",
    "\n",
    "> v = x1w1 + x2w2\n",
    "\n",
    "### ## Forward Pass\n",
    "<img src=\"assets/pic18.PNG\" style=\"width:250px\"/>\n",
    "\n",
    "> x1 = -3; w1 = 3\n",
    "\n",
    "> x2 = 4; w2 =2\n",
    "\n",
    "> a = x1 * w1\n",
    "    = -9\n",
    "    \n",
    "> b = x2 * w2\n",
    "    = 8\n",
    "\n",
    "> v = a + b\n",
    "    = -1\n",
    "    \n",
    "> ùë¶¬†ÃÇ= 1/(1+ùëí^(‚àíùë£) )\n",
    "    = 0.27\n",
    "    \n",
    "### ## Loss Gradient\n",
    "<img src=\"assets/pic18.PNG\" style=\"width:250px\"/>\n",
    "\n",
    "> Calculate Loss Function\n",
    "    - Example gradient:\n",
    "<img src=\"assets/pic20.PNG\" style=\"width:70px\"/>\n",
    "> What we need:\n",
    "<img src=\"assets/pic21.PNG\" style=\"width:75px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"assets/pic22.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"assets/pic23.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"assets/pic24.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"assets/pic25.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "---\n",
    "<img src=\"assets/pic26.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Chain Rule\n",
    "<img src=\"assets/pic27.PNG\" style=\"width:250px\"/>\n",
    "<img src=\"assets/pic28.PNG\" style=\"width:300px\"/>\n",
    "\n",
    "### ## Chain Rule Vectorized\n",
    "<img src=\"assets/pic29.PNG\" style=\"width:550px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient in Neural Network\n",
    "---\n",
    "\n",
    "### ## Single Layer Derivative, Easy\n",
    "\n",
    "<img src=\"assets/pic30.PNG\" style=\"width:400px\"/>\n",
    "\n",
    "### ## Multi Lyaer Derivative ??\n",
    "\n",
    "<img src=\"assets/pic31.PNG\" style=\"width:450px\"/>\n",
    "\n",
    "### ## MLPs =  Stacks of Linear Functions\n",
    "* Each layer use the same function\n",
    "    - Remember that when calculating gradient, we get dW and dX\n",
    "<img src=\"assets/pic32.PNG\" style=\"width:550px\"/>\n",
    "\n",
    "### ## Back Propagate the Gradient\n",
    "* Calculate gradient one-by one\n",
    "    - Output gradient =  external gradient * local gradient\n",
    "<img src=\"assets/pic33.PNG\" style=\"width:550px\"/>\n",
    "\n",
    "### ## This is Called: Backpropagation\n",
    "<img src=\"assets/pic34.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "### ## Backpropagation\n",
    "* Backward propagation of errors\n",
    "* Calculating gradient of weights in the stacked functions by folowing gradient error that is computed at the output end and distributed backwards throught the network's layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient Descent\n",
    "---\n",
    "\n",
    "##### * Optimization Problem: \n",
    "- generalization problem to minimize error for all data\n",
    "\n",
    "##### * Gradient:\n",
    "- a slope of which way the parameters must go to minimize the error (based on current data)\n",
    "\n",
    "##### * Gradient Descent\n",
    "- a first-order iterative optimization algorithm for finding the minimum of a function\n",
    "\n",
    "### ## Gradient descent Variants\n",
    "\n",
    "##### * Vanilla Gradient Descent/Full Batch Gradient Descent\n",
    "\n",
    "<img src=\"assets/pic39.PNG\" style=\"width:120px\"/>\n",
    "\n",
    "* Use all ùëö data in each iteration\n",
    "* 1 epoch = 1 iteration\n",
    "    > Problem:\n",
    "    * memory size\n",
    "    * Took long time to update\n",
    "\n",
    "##### * Stochastic Gradient Descent\n",
    "\n",
    "<img src=\"assets/pic40.PNG\" style=\"width:180px\"/>\n",
    "\n",
    "* Use only 1 training data in each iteration\n",
    "* For ùëö data in training set\n",
    "    - 1 epoch = ùëö iteration\n",
    "* Weight updated more often\n",
    "    > Problem:\n",
    "    * Each data resulting different gradient\n",
    "    * Each epoch may takes longer\n",
    "    * May not converge when it‚Äôs close to optimum\n",
    "\n",
    "<img src=\"assets/pic37.PNG\" style=\"width:500px\"/>\n",
    "\n",
    "##### * Mini-Batch Gradient Descent\n",
    "\n",
    "<img src=\"assets/pic41.PNG\" style=\"width:240px\"/>\n",
    "\n",
    "* Use some data (ùëè data) in each iteration\n",
    "* use however many you can\n",
    "* Weight updated more often\n",
    "* Loss is not as scattered as SGD\n",
    "* Converge better and faster than SGD\n",
    "    \n",
    "<img src=\"assets/pic38.PNG\" style=\"width:400px\"/>\n",
    "\n",
    "##### <> Gradient Descent Benchmark\n",
    "\n",
    "<img src=\"assets/pic42.PNG\" style=\"width:350px\"/>\n",
    "\n",
    "### ## Iteration != Epoch\n",
    "\n",
    "> 1 iteration (1 step)\n",
    "* one time the algorithm performs: \n",
    "    * forward propagation, \n",
    "    * error calculation, \n",
    "    * back propagation, and \n",
    "    * weight update\n",
    "\n",
    "> 1 epoch\n",
    "* The algorithm has iterated all the data in the training set\n",
    "* Each data has been used in forward and backward pass\n",
    "\n",
    "### ## Shuffling the Data\n",
    "\n",
    "* Avoid providing the training examples in a meaningful order\n",
    "    - It may bias the optimation algorithm\n",
    "* Shuffle the training data after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
