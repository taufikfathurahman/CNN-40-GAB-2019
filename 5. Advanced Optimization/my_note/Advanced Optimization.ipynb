{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Parameter Update Schemes\n",
    "\n",
    "---\n",
    "\n",
    "### ## Vanilla Update (SGD)\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100077-b93aa800-e5d3-11e9-98f1-36308c9bd5ef.png\" width=\"500px\"></p>\n",
    "\n",
    "#### ### Problem with SGD\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100233-51d12800-e5d4-11e9-81a2-ea9785d622f8.png\" width=\"500px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100213-3a923a80-e5d4-11e9-983b-98bfa1bb461c.png\" width=\"500px\"></p>\n",
    "\n",
    "#### ### Local Minima vs Saddle Point\n",
    "\n",
    "* Local Minima means:\n",
    "    * For all parametersâ€™ direction/change, the loss increases\n",
    "    * For thousand parameters in NeuralNet, it rarely happen\n",
    "\n",
    "* Saddle Point means:\n",
    "    * For some parametersâ€™ direction/change, the loss does not change\n",
    "    * It happened almost everywhere\n",
    "    * Itâ€™s worse\n",
    "    \n",
    "#### ### Vanilla Update (SGD) Challenges:\n",
    "* Learning rate affect all parameters equally\n",
    "* Choosing a proper learning rate can be difficult\n",
    "* Better define learning rate decaying schedules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Momentum Update\n",
    "\n",
    "---\n",
    "\n",
    "* Version A\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100297-98268700-e5d4-11e9-85ca-15b0cfb7cf55.png\" width=\"200px\"></p>\n",
    "* Version B\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100300-9a88e100-e5d4-11e9-86b3-35979aa36eec.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66119783-94f6bf80-e603-11e9-968c-8ebdc25e8d28.png\" width=\"400px\"></p>\n",
    "\n",
    "* Inspired by Physical interpretation of friction\n",
    "    * Imagine gradient as a ball rolling down in hilly terrain\n",
    "        * (ğœ‡ ğ‘£_(ğ‘¡âˆ’1))  --> friction\n",
    "        * (ğ›¼ğ›»ğ‘“(ğ‘Š_ğ‘¡))  --> force, acceleration\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120093-43026980-e604-11e9-8f5b-3bd7a813107a.png\" width=\"400px\"></p>\n",
    "\n",
    "* Allows a velocity to \"build up\" along shallow directions\n",
    "    * The loss overshoots the target at first, \n",
    "    * then converge back quickly\n",
    "* Velocity damped in steep direction\n",
    "    * Due to quickly changing sign\n",
    "* Almost always result better converge rates on deep network\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120233-9e345c00-e604-11e9-9ba6-0a5061eabdd5.png\" width=\"400px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Neterov-Accelerated Gradient Momentum Update\n",
    "\n",
    "---\n",
    "\n",
    "* Instead calculating gradient at point of ( ğ‘¥ ), compute the future approximate position of ( ğ‘¥+ğ›¼ğ›»ğ‘“(ğ‘Š)) as a \"lookahead\"\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120335-d63b9f00-e604-11e9-8a98-3a07e14d5f9f.png\" width=\"400px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120406-f8cdb800-e604-11e9-8be9-cbcca902b95b.png\" width=\"200px\"></p>\n",
    "\n",
    "* Instead of only calculating backward pass of ğ›»ğ‘“(ğ‘Š), we also have to calculate backward pass of the lookahead\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120494-2fa3ce00-e605-11e9-90ab-346d35ba9985.png\" width=\"400px\"></p>\n",
    "\n",
    "* Transform and rearrange\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120611-75f92d00-e605-11e9-8537-e2b437cb5983.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120770-e1db9580-e605-11e9-845a-c29f41c27074.png\" width=\"400px\"></p>\n",
    "\n",
    "* We have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120621-78f41d80-e605-11e9-8953-80959b543d96.png\" width=\"150px\"></p>\n",
    "* Thus\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120623-7bef0e00-e605-11e9-9efa-53eca55859fc.png\" width=\"200px\"></p>\n",
    "\n",
    "* Stronger theoretical converge guarantees for convex functions\n",
    "* Consistently works slightly better than standard momentum\n",
    "    * Almost always converge faster\n",
    "    * Because of the lookahead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Sub-gradient Update\n",
    "\n",
    "---\n",
    "\n",
    "* Per-parameter adaptive learning rate methods\n",
    "    * adaptively tune the learning rates\n",
    "\n",
    "* Added element-wise scaling of the gradient based on the historical sum of squares in each dimension\n",
    "    * used to normalize the parameter update step, element-wise\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121125-a097b580-e606-11e9-9667-8dc4377b4b73.png\" width=\"400px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121177-c02ede00-e606-11e9-8e93-d20f146fdc5e.png\" width=\"400px\"></p>\n",
    "\n",
    "* Benefit\n",
    "    * the weights that receive high gradients will have their effective learning rate reduced, \n",
    "    * while weights that receive small gradients or infrequent updates will have their effective learning rate increased\n",
    "* Downside\n",
    "    * in case of Deep Learning, the monotonic learning rate usually proves too aggressive and stops learning too early\n",
    "        * Because the cache build up over time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Root Mean Square Propagation Update\n",
    "\n",
    "---\n",
    "\n",
    "* Introduced to adjusts the Adagrad method in a very simple way \n",
    "    * an attempt to reduce its aggressive and monotonically decreasing learning rate\n",
    "    * uses a moving average of squared gradients\n",
    "\n",
    "* Instead of keeping the sum of square completely, \u000b",
    "use a leaky counter \n",
    "* AdaGrad Cache\n",
    "    * Accumulate all second moment gradient\n",
    "* Leaky Counter\n",
    "    * Decay (forget) previous buildup\n",
    "    * Add small portion of current second moment\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121397-49deab80-e607-11e9-959a-a5304c381762.png\" width=\"200px\"></p>\n",
    "\n",
    "* Instead of keeping the sum of square completely, use a leaky counter \n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121441-6da1f180-e607-11e9-8307-654b8fbcf0a2.png\" width=\"400px\"></p>\n",
    "\n",
    "* The effect:\n",
    "    * Still maintain the nice equalizing effect of step sizes in steep or shallow dimension\n",
    "    * The learning rate wonâ€™t converge completely to zero\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121606-c5d8f380-e607-11e9-87eb-2dfa3f950197.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121610-c83b4d80-e607-11e9-8c66-970377e5e316.png\" width=\"400px\"></p>\n",
    "\n",
    "* Usually in practice, \n",
    "    * AdaGrad tends to stops too early \n",
    "    * while RMSProp will end up converge better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Delta Gradient Update\n",
    "\n",
    "---\n",
    "\n",
    "* An extension of AdaGrad \n",
    "    * Also an attempt to reduce its aggressive and monotonically decreasing learning rate\n",
    "    * Restricts the window of accumulated past gradients to some fixed size w\n",
    "* Similar to RMSProp\n",
    "    * RMSProp and AdaDelta have both been developed independently around the same time\n",
    "* Decay the gradient bildup\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121903-7fd05f80-e608-11e9-9fd5-7a13c1520362.png\" width=\"200px\"></p>\n",
    "\n",
    "* The authors note that the units ğ›¼ in this update do not have the same hypothetical units as the parameter\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121908-8232b980-e608-11e9-9cd1-07eaa239490f.png\" width=\"200px\"></p>\n",
    "\n",
    "* Introducing ğ‘¥_ğ‘¡ replacing learning rate\n",
    "* Define another exponentially decaying average for parameter update\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121972-b8703900-e608-11e9-8f73-84c3089a5591.png\" width=\"200px\"></p>\n",
    "\n",
    "* Thus we have the AdaDelta update rule\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66122077-0422e280-e609-11e9-9e13-1c7fcc1fcee4.png\" width=\"250px\"></p>\n",
    "\n",
    "* do not even need to set a default learning rate, as it has been eliminated from the update rule\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66124179-ee63ec00-e60d-11e9-8ca7-e3a1d38c1e7e.png\" width=\"400px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Moment Estimator Update\n",
    "\n",
    "---\n",
    "\n",
    "* A recently proposed update that looks a bit like RMSProp combined with Momentum\n",
    "* Using â€œsmoothâ€ version of the gradient ğ‘š \n",
    "    * instead of the raw (and perhaps noisy) gradient vector ğ‘‘ğ‘¥\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66124179-ee63ec00-e60d-11e9-8ca7-e3a1d38c1e7e.png\" width=\"400px\"></p>\n",
    "\n",
    "* Momentum build up the gradient (first moment) to estimate the mean\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66126748-5a495300-e614-11e9-884d-190b609d7fcf.png\" width=\"200px\"></p>\n",
    "\n",
    "* RMSProp build up the squared gradient (second moment) to estimate the uncentered variance\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66126695-43a2fc00-e614-11e9-8a66-9d3966d34927.png\" width=\"200px\"></p>\n",
    "\n",
    "* Change the Momentum slightly to use decay rate to stabilize the gradient, and we have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127089-202c8100-e615-11e9-8a74-4fce35d1f995.png\" width=\"220px\"></p>\n",
    "\n",
    "    * Velocity in Momentum (ğ‘£_ğ‘¡) --> mean estimation (ğ‘š_ğ‘¡)\n",
    "    * Gradient cache in RMSProp (ğ‘”_ğ‘¡) --> variance estimation (ğ‘£_ğ‘¡)\n",
    "* And the update rule become\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127097-23c00800-e615-11e9-9c1a-1e733eb25002.png\" width=\"180px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127182-51a54c80-e615-11e9-8f01-395c9bb1a74d.png\" width=\"400px\"></p>\n",
    "\n",
    "### ## Bias Correction\n",
    "\n",
    "* First and second moment estimates start at zero\n",
    "* ğ›½_1 and ğ›½_2 are typically very close to 1 (0.9 or 0.99)\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127477-f6c02500-e615-11e9-9266-f5bc169f6777.png\" width=\"400px\"></p>\n",
    "\n",
    "* At the first update\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127545-266f2d00-e616-11e9-8b17-c781457d90b1.png\" width=\"400px\"></p>\n",
    "\n",
    "* Incorrect statistic in the beginning, Artifact from zero initialization\n",
    "* Compensate first and second moment by step by scaling up according to the current timestep t\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127631-59192580-e616-11e9-98fb-4a5973e8fc13.png\" width=\"350px\"></p>\n",
    "\n",
    "* Only changing while Adam is warming up for the first few iterations\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127692-806ff280-e616-11e9-9b18-2698c0c2aae3.png\" width=\"300px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127733-92ea2c00-e616-11e9-876e-4051ae5a9a09.png\" width=\"400px\"></p>\n",
    "\n",
    "---\n",
    "\n",
    "- Often works slightly better than RMSProp\n",
    "- Hyperparameters are less sensitive\n",
    "    - ğ›½_1=0.9 , ğ›½_2=0.99\n",
    "    - ğ›¼=1ğ‘’âˆ’3 \" or \" 5ğ‘’âˆ’4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # AdaMax Update\n",
    "\n",
    "---\n",
    "\n",
    "* The ğ‘£_ğ‘¡ factor in the Adam update rule scales the gradient inversely proportionally to the ğ¿2 norm of the past gradients and current gradient ğ›»ğ‘“(ğ‘Š_ğ‘¡ )^2\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129158-903d0600-e619-11e9-8498-e9a4bad719a5.png\" width=\"250px\"></p>\n",
    "\n",
    "* We can generalize this update to the â„“_ğ‘\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129189-a054e580-e619-11e9-9038-473bf74d9603.png\" width=\"250px\"></p>\n",
    "\n",
    "* Norms for large ğ‘ values generally become numerically unstable, \n",
    "* Which is why ğ¿1 and ğ¿2 norms are most common in practice\n",
    "* However, ğ¿âˆ also generally exhibits stable behavior\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129490-3721a200-e61a-11e9-8684-f24da9980300.png\" width=\"250px\"></p>\n",
    "\n",
    "    - To avoid confusion with Adam, we use ğ‘¢_ğ‘¡ to denote the infinity norm-constrained ğ‘£_ğ‘¡\n",
    "* From the ğ¿âˆ formula, \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129693-9f708380-e61a-11e9-9a04-c0d8cfd1eba9.png\" width=\"250px\"></p>\n",
    "\n",
    "* The authors propose AdaMax and show that ğ‘£_ğ‘¡ with ğ¿âˆ converges to the following more stable formula\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129721-abf4dc00-e61a-11e9-9168-12b92360c6a1.png\" width=\"250px\"></p>\n",
    "\n",
    "* Using the new variance estimation,\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130237-9633e680-e61b-11e9-97db-9c33c09f7516.png\" width=\"250px\"></p>\n",
    "\n",
    "* Plug into Adam update equation by replacing âˆš(ğ‘£Â Ì‚_ğ‘¡ )+ğœ– to obtain the AdaMax update rule\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130242-98964080-e61b-11e9-8ddc-3d929f28f306.png\" width=\"200px\"></p>\n",
    "\n",
    "* As ğ‘¢_ğ‘¡ relies on the max operation, it no longer needs bias correction, but ğ‘š_ğ‘¡ still does\n",
    "* Thus the complete equation of AdaMax is\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130367-d1ceb080-e61b-11e9-938c-10808eaec2e5.png\" width=\"350px\"></p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Nadam Update\n",
    "\n",
    "---\n",
    "\n",
    "* Adam \n",
    "    * a combination of RMSprop and Momentum\n",
    "    * Momentum accounts for the exponentially decaying average of past gradients ğ‘š_ğ‘¡  \n",
    "    * RMSprop contributes the exponentially decaying average of past squared gradients ğ‘£_ğ‘¡\n",
    "* Nesterov Accelerated Gradient (NAG) \n",
    "    * superior to vanilla Momentum\n",
    "* Nesterov-accelerated Adaptive Moment Estimation\n",
    "    * a combination of RMSprop and Nesterov-Accelerated\n",
    "* Recall \n",
    "    * Momentum update:\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131384-8ae1ba80-e61d-11e9-84b1-1d78b704e613.png\" width=\"250px\"></p>\n",
    "    \n",
    "    * And NAG update:\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131426-99c86d00-e61d-11e9-97f5-5311d8f67058.png\" width=\"250px\"></p>\n",
    "\n",
    "* Do the same in Adam equation, modify ğ‘šÂ Ì‚_ğ‘¡  to compensate the lookahead, and we have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131610-f166d880-e61d-11e9-9cc1-e5401ab94b83.png\" width=\"400px\"></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
