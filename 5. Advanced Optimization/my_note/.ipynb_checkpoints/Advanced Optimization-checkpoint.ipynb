{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Parameter Update Schemes\n",
    "\n",
    "---\n",
    "\n",
    "### ## Vanilla Update (SGD)\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100077-b93aa800-e5d3-11e9-98f1-36308c9bd5ef.png\" width=\"500px\"></p>\n",
    "\n",
    "#### ### Problem with SGD\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100233-51d12800-e5d4-11e9-81a2-ea9785d622f8.png\" width=\"500px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100213-3a923a80-e5d4-11e9-983b-98bfa1bb461c.png\" width=\"500px\"></p>\n",
    "\n",
    "#### ### Local Minima vs Saddle Point\n",
    "\n",
    "* Local Minima means:\n",
    "    * For all parametersâ€™ direction/change, the loss increases\n",
    "    * For thousand parameters in NeuralNet, it rarely happen\n",
    "\n",
    "* Saddle Point means:\n",
    "    * For some parametersâ€™ direction/change, the loss does not change\n",
    "    * It happened almost everywhere\n",
    "    * Itâ€™s worse\n",
    "    \n",
    "#### ### Vanilla Update (SGD) Challenges:\n",
    "* Learning rate affect all parameters equally\n",
    "* Choosing a proper learning rate can be difficult\n",
    "* Better define learning rate decaying schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Momentum Update\n",
    "\n",
    "---\n",
    "\n",
    "* Version A\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100297-98268700-e5d4-11e9-85ca-15b0cfb7cf55.png\" width=\"200px\"></p>\n",
    "* Version B\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66100300-9a88e100-e5d4-11e9-86b3-35979aa36eec.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66119783-94f6bf80-e603-11e9-968c-8ebdc25e8d28.png\" width=\"400px\"></p>\n",
    "\n",
    "* Inspired by Physical interpretation of friction\n",
    "    * Imagine gradient as a ball rolling down in hilly terrain\n",
    "        * (ğœ‡ ğ‘£_(ğ‘¡âˆ’1))  --> friction\n",
    "        * (ğ›¼ğ›»ğ‘“(ğ‘Š_ğ‘¡))  --> force, acceleration\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120093-43026980-e604-11e9-8f5b-3bd7a813107a.png\" width=\"400px\"></p>\n",
    "\n",
    "* Allows a velocity to \"build up\" along shallow directions\n",
    "    * The loss overshoots the target at first, \n",
    "    * then converge back quickly\n",
    "* Velocity damped in steep direction\n",
    "    * Due to quickly changing sign\n",
    "* Almost always result better converge rates on deep network\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120233-9e345c00-e604-11e9-9ba6-0a5061eabdd5.png\" width=\"400px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Neterov-Accelerated Gradient Momentum Update\n",
    "\n",
    "---\n",
    "\n",
    "* Instead calculating gradient at point of ( ğ‘¥ ), compute the future approximate position of ( ğ‘¥+ğ›¼ğ›»ğ‘“(ğ‘Š)) as a \"lookahead\"\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120335-d63b9f00-e604-11e9-8a98-3a07e14d5f9f.png\" width=\"400px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120406-f8cdb800-e604-11e9-8be9-cbcca902b95b.png\" width=\"200px\"></p>\n",
    "\n",
    "* Instead of only calculating backward pass of ğ›»ğ‘“(ğ‘Š), we also have to calculate backward pass of the lookahead\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120494-2fa3ce00-e605-11e9-90ab-346d35ba9985.png\" width=\"400px\"></p>\n",
    "\n",
    "* Transform and rearrange\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120611-75f92d00-e605-11e9-8537-e2b437cb5983.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120770-e1db9580-e605-11e9-845a-c29f41c27074.png\" width=\"400px\"></p>\n",
    "\n",
    "* We have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120621-78f41d80-e605-11e9-8953-80959b543d96.png\" width=\"150px\"></p>\n",
    "* Thus\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66120623-7bef0e00-e605-11e9-9efa-53eca55859fc.png\" width=\"200px\"></p>\n",
    "\n",
    "* Stronger theoretical converge guarantees for convex functions\n",
    "* Consistently works slightly better than standard momentum\n",
    "    * Almost always converge faster\n",
    "    * Because of the lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Sub-gradient Update\n",
    "\n",
    "---\n",
    "\n",
    "* Per-parameter adaptive learning rate methods\n",
    "    * adaptively tune the learning rates\n",
    "\n",
    "* Added element-wise scaling of the gradient based on the historical sum of squares in each dimension\n",
    "    * used to normalize the parameter update step, element-wise\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121125-a097b580-e606-11e9-9667-8dc4377b4b73.png\" width=\"400px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121177-c02ede00-e606-11e9-8e93-d20f146fdc5e.png\" width=\"400px\"></p>\n",
    "\n",
    "* Benefit\n",
    "    * the weights that receive high gradients will have their effective learning rate reduced, \n",
    "    * while weights that receive small gradients or infrequent updates will have their effective learning rate increased\n",
    "* Downside\n",
    "    * in case of Deep Learning, the monotonic learning rate usually proves too aggressive and stops learning too early\n",
    "        * Because the cache build up over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Root Mean Square Propagation Update\n",
    "\n",
    "---\n",
    "\n",
    "* Introduced to adjusts the Adagrad method in a very simple way \n",
    "    * an attempt to reduce its aggressive and monotonically decreasing learning rate\n",
    "    * uses a moving average of squared gradients\n",
    "\n",
    "* Instead of keeping the sum of square completely, \u000b",
    "use a leaky counter \n",
    "* AdaGrad Cache\n",
    "    * Accumulate all second moment gradient\n",
    "* Leaky Counter\n",
    "    * Decay (forget) previous buildup\n",
    "    * Add small portion of current second moment\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121397-49deab80-e607-11e9-959a-a5304c381762.png\" width=\"200px\"></p>\n",
    "\n",
    "* Instead of keeping the sum of square completely, use a leaky counter \n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121441-6da1f180-e607-11e9-8307-654b8fbcf0a2.png\" width=\"400px\"></p>\n",
    "\n",
    "* The effect:\n",
    "    * Still maintain the nice equalizing effect of step sizes in steep or shallow dimension\n",
    "    * The learning rate wonâ€™t converge completely to zero\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121606-c5d8f380-e607-11e9-87eb-2dfa3f950197.png\" width=\"200px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121610-c83b4d80-e607-11e9-8c66-970377e5e316.png\" width=\"400px\"></p>\n",
    "\n",
    "* Usually in practice, \n",
    "    * AdaGrad tends to stops too early \n",
    "    * while RMSProp will end up converge better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Delta Gradient Update\n",
    "\n",
    "---\n",
    "\n",
    "* An extension of AdaGrad \n",
    "    * Also an attempt to reduce its aggressive and monotonically decreasing learning rate\n",
    "    * Restricts the window of accumulated past gradients to some fixed size w\n",
    "* Similar to RMSProp\n",
    "    * RMSProp and AdaDelta have both been developed independently around the same time\n",
    "* Decay the gradient bildup\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121903-7fd05f80-e608-11e9-9fd5-7a13c1520362.png\" width=\"200px\"></p>\n",
    "\n",
    "* The authors note that the units ğ›¼ in this update do not have the same hypothetical units as the parameter\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121908-8232b980-e608-11e9-9cd1-07eaa239490f.png\" width=\"200px\"></p>\n",
    "\n",
    "* Introducing ğ‘¥_ğ‘¡ replacing learning rate\n",
    "* Define another exponentially decaying average for parameter update\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66121972-b8703900-e608-11e9-8f73-84c3089a5591.png\" width=\"200px\"></p>\n",
    "\n",
    "* Thus we have the AdaDelta update rule\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66122077-0422e280-e609-11e9-9e13-1c7fcc1fcee4.png\" width=\"250px\"></p>\n",
    "\n",
    "* do not even need to set a default learning rate, as it has been eliminated from the update rule\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66124179-ee63ec00-e60d-11e9-8ca7-e3a1d38c1e7e.png\" width=\"400px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Adaptive Moment Estimator Update\n",
    "\n",
    "---\n",
    "\n",
    "* A recently proposed update that looks a bit like RMSProp combined with Momentum\n",
    "* Using â€œsmoothâ€ version of the gradient ğ‘š \n",
    "    * instead of the raw (and perhaps noisy) gradient vector ğ‘‘ğ‘¥\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66124179-ee63ec00-e60d-11e9-8ca7-e3a1d38c1e7e.png\" width=\"400px\"></p>\n",
    "\n",
    "* Momentum build up the gradient (first moment) to estimate the mean\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66126748-5a495300-e614-11e9-884d-190b609d7fcf.png\" width=\"200px\"></p>\n",
    "\n",
    "* RMSProp build up the squared gradient (second moment) to estimate the uncentered variance\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66126695-43a2fc00-e614-11e9-8a66-9d3966d34927.png\" width=\"200px\"></p>\n",
    "\n",
    "* Change the Momentum slightly to use decay rate to stabilize the gradient, and we have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127089-202c8100-e615-11e9-8a74-4fce35d1f995.png\" width=\"220px\"></p>\n",
    "\n",
    "    * Velocity in Momentum (ğ‘£_ğ‘¡) --> mean estimation (ğ‘š_ğ‘¡)\n",
    "    * Gradient cache in RMSProp (ğ‘”_ğ‘¡) --> variance estimation (ğ‘£_ğ‘¡)\n",
    "* And the update rule become\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127097-23c00800-e615-11e9-9c1a-1e733eb25002.png\" width=\"180px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127182-51a54c80-e615-11e9-8f01-395c9bb1a74d.png\" width=\"400px\"></p>\n",
    "\n",
    "### ## Bias Correction\n",
    "\n",
    "* First and second moment estimates start at zero\n",
    "* ğ›½_1 and ğ›½_2 are typically very close to 1 (0.9 or 0.99)\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127477-f6c02500-e615-11e9-9266-f5bc169f6777.png\" width=\"400px\"></p>\n",
    "\n",
    "* At the first update\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127545-266f2d00-e616-11e9-8b17-c781457d90b1.png\" width=\"400px\"></p>\n",
    "\n",
    "* Incorrect statistic in the beginning, Artifact from zero initialization\n",
    "* Compensate first and second moment by step by scaling up according to the current timestep t\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127631-59192580-e616-11e9-98fb-4a5973e8fc13.png\" width=\"350px\"></p>\n",
    "\n",
    "* Only changing while Adam is warming up for the first few iterations\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127692-806ff280-e616-11e9-9b18-2698c0c2aae3.png\" width=\"300px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66127733-92ea2c00-e616-11e9-876e-4051ae5a9a09.png\" width=\"400px\"></p>\n",
    "\n",
    "---\n",
    "\n",
    "- Often works slightly better than RMSProp\n",
    "- Hyperparameters are less sensitive\n",
    "    - ğ›½_1=0.9 , ğ›½_2=0.99\n",
    "    - ğ›¼=1ğ‘’âˆ’3 \" or \" 5ğ‘’âˆ’4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # AdaMax Update\n",
    "\n",
    "---\n",
    "\n",
    "* The ğ‘£_ğ‘¡ factor in the Adam update rule scales the gradient inversely proportionally to the ğ¿2 norm of the past gradients and current gradient ğ›»ğ‘“(ğ‘Š_ğ‘¡ )^2\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129158-903d0600-e619-11e9-8498-e9a4bad719a5.png\" width=\"250px\"></p>\n",
    "\n",
    "* We can generalize this update to the â„“_ğ‘\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129189-a054e580-e619-11e9-9038-473bf74d9603.png\" width=\"250px\"></p>\n",
    "\n",
    "* Norms for large ğ‘ values generally become numerically unstable, \n",
    "* Which is why ğ¿1 and ğ¿2 norms are most common in practice\n",
    "* However, ğ¿âˆ also generally exhibits stable behavior\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129490-3721a200-e61a-11e9-8684-f24da9980300.png\" width=\"250px\"></p>\n",
    "\n",
    "    - To avoid confusion with Adam, we use ğ‘¢_ğ‘¡ to denote the infinity norm-constrained ğ‘£_ğ‘¡\n",
    "* From the ğ¿âˆ formula, \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129693-9f708380-e61a-11e9-9a04-c0d8cfd1eba9.png\" width=\"250px\"></p>\n",
    "\n",
    "* The authors propose AdaMax and show that ğ‘£_ğ‘¡ with ğ¿âˆ converges to the following more stable formula\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66129721-abf4dc00-e61a-11e9-9168-12b92360c6a1.png\" width=\"250px\"></p>\n",
    "\n",
    "* Using the new variance estimation,\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130237-9633e680-e61b-11e9-97db-9c33c09f7516.png\" width=\"250px\"></p>\n",
    "\n",
    "* Plug into Adam update equation by replacing âˆš(ğ‘£Â Ì‚_ğ‘¡ )+ğœ– to obtain the AdaMax update rule\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130242-98964080-e61b-11e9-8ddc-3d929f28f306.png\" width=\"200px\"></p>\n",
    "\n",
    "* As ğ‘¢_ğ‘¡ relies on the max operation, it no longer needs bias correction, but ğ‘š_ğ‘¡ still does\n",
    "* Thus the complete equation of AdaMax is\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66130367-d1ceb080-e61b-11e9-938c-10808eaec2e5.png\" width=\"350px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Nadam Update\n",
    "\n",
    "---\n",
    "\n",
    "* Adam \n",
    "    * a combination of RMSprop and Momentum\n",
    "    * Momentum accounts for the exponentially decaying average of past gradients ğ‘š_ğ‘¡  \n",
    "    * RMSprop contributes the exponentially decaying average of past squared gradients ğ‘£_ğ‘¡\n",
    "* Nesterov Accelerated Gradient (NAG) \n",
    "    * superior to vanilla Momentum\n",
    "* Nesterov-accelerated Adaptive Moment Estimation\n",
    "    * a combination of RMSprop and Nesterov-Accelerated\n",
    "* Recall \n",
    "    * Momentum update:\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131384-8ae1ba80-e61d-11e9-84b1-1d78b704e613.png\" width=\"250px\"></p>\n",
    "    \n",
    "    * And NAG update:\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131426-99c86d00-e61d-11e9-97f5-5311d8f67058.png\" width=\"250px\"></p>\n",
    "\n",
    "* Do the same in Adam equation, modify ğ‘šÂ Ì‚_ğ‘¡  to compensate the lookahead, and we have\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66131610-f166d880-e61d-11e9-9cc1-e5401ab94b83.png\" width=\"400px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Transfer Learning\n",
    "\n",
    "---\n",
    "\n",
    "* It takes a lot of data to train Deep Neural Net / Convolutional Neural Net\n",
    "* Lots of parameters (weights) must be trained\n",
    "* Too little data will make the network overfit\n",
    "* What if we only have small number of data?\n",
    "* The answer is: \n",
    "    * **TRANSFER LEARNING**\n",
    "    * **FINE-TUNING**\n",
    "* We know that Convolution and Pooling layers act as a big feature extraction that feeds the classic Neural Net as a classifier\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257533-b2bf5280-e7c4-11e9-8925-074346acb37f.png\" width=\"500px\"></p>\n",
    "\n",
    "* Closer to the input, the weights represent more generic features\n",
    "    * Dots, colors, edges,\n",
    "* Closer to the softmax output, the weights represent more specific and complex features\n",
    "    * Object parts\n",
    "* Define the pretrained feature needed according to your case\n",
    "* Things to consider:\n",
    "    * How much data do we have\n",
    "    * How far is the pattern / type of data from the pretrained model (how far from the ImageNet data pattern)\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257558-10ec3580-e7c5-11e9-8bec-a670b0b7074d.png\" width=\"130px\"></p>\n",
    "\n",
    "### ## ConvNet as Feature Extraction\n",
    "\n",
    "* Take the model and weight of the ConvNet network that has been trained with ImageNet\n",
    "* Remove/delete the last softmax and some or even all FC layers\n",
    "* Set the remaining network so that it cannot be trained (freeze)\n",
    "* Install the new \"head\" (FC layer and softmax) as needed\n",
    "* Train the new network with your dataset\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257601-74766300-e7c5-11e9-9032-899bf0962da1.png\" width=\"130px\"></p>\n",
    "\n",
    "* By regulating that some layers cannot be trained, the backward pass will only occur in layers that are not frozen\n",
    "* Just as we only train a few layers of vanilla Artificial Neural Network with ConvNet as a Feature Extraction\n",
    "* If you have medium-sized data\n",
    "* Or if your data is quite different from ImageNet\n",
    "* Delete or retrain a deeper layer\n",
    "* Or Install a new \"head\" (FC layer and softmax) somewhere in the middle\n",
    "* Or even train the whole network\n",
    "* Typically, use lower learning rate when finetuning (~1/10 of original LR)\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257625-cfa85580-e7c5-11e9-997e-12b648b3c86f.png\" width=\"130px\"></p>\n",
    "\n",
    "### ## When and How to fine-tune\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257644-0b431f80-e7c6-11e9-806f-3e4c4133c5c7.png\" width=\"450px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Unsupervised Pretraining\n",
    "\n",
    "---\n",
    "\n",
    "* If your dataset is small (less than 1 million images) and very different\n",
    "    * (a) Look for a dataset that is similar and has a large number\n",
    "    * Train ConvNet to classify that dataset\n",
    "    * Or train ConvNet with the AutoEncoder scheme\n",
    "    * After converging enough, transfer learning ConvNet for your dataset\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    * Or even (b) find several similar datasets \n",
    "    * Join them together into a giant dataset\n",
    "    * Train ConvNet with the AutoEncoder scheme\n",
    "    * After converging enough, transfer learning ConvNet for your dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Model Ensembles\n",
    "\n",
    "---\n",
    "\n",
    "* A learning paradigm where many neural networks are jointly used to solve a problem\n",
    "* Generating multiple versions of a predictor network and using them to get an aggregated prediction\n",
    "\n",
    "\n",
    "**### Model Ensembles Steps:**\n",
    "* Train multiple independent models\n",
    "* At test time average their results\n",
    "* Enjoy 2% extra performance\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257797-940e8b00-e7c7-11e9-918f-4cfe0635f32c.png\" width=\"450px\"></p>\n",
    "\n",
    "* **Bagging** â€“ Bootstrap Aggregating\n",
    "    * Generates several training sets from the original training set (randomly drawn subset) \n",
    "    * then trains a component neural network from each of those training sets\n",
    "    * Downside: lots of model to train\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257839-1b5bfe80-e7c8-11e9-8dae-67d0dd63c980.png\" width=\"300px\"></p>\n",
    "\n",
    "* **Boosting**\n",
    "    * Incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models misclassified\n",
    "    * Downside: with mini-batch and millions of data, sometimes it does not help\n",
    "\n",
    "* **Different epochs**\n",
    "    * Multiple models trained with different maximum epoch\n",
    "    * Combining models before and after it overfits the data\n",
    "\n",
    "* **Different checkpoints**\n",
    "    * Use a singe model\n",
    "    * Set check point over time (every after n-epoch) and get the network at that point\n",
    "    \n",
    "* **Different Initialization**\n",
    "    * cross-validation to determine the best hyperparameters\n",
    "    * train multiple models with the best set of hyperparameters but with different random initialization. \n",
    "    * Downside : the variety is only due to initialization.\n",
    "\n",
    "* **Random Forest**\n",
    "\n",
    "* **Running average of parameters during training**\n",
    "    * Maintain a second copy of the networkâ€™s weights in memory with exponentially decaying sum of previous weights during training\n",
    "    * Averaging the state of the network over last several iterations\n",
    "    * Use the maintained weight in test time\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257917-2bc0a900-e7c9-11e9-8c6f-6cc0870a2efc.png\" width=\"500px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Dropout Regularization\n",
    "\n",
    "---\n",
    "\n",
    "### ## Dropout\n",
    "\n",
    "* Randomly set some neurons to zero during forward pass for each epoch\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257943-878b3200-e7c9-11e9-9f9b-9a3d834f9d5c.png\" width=\"400px\"></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257952-a1c51000-e7c9-11e9-8ea9-fb727c5a1320.png\" width=\"500px\"></p>\n",
    "\n",
    "### ## Why Dropout?\n",
    "\n",
    "* Prevents overfitting\n",
    "* Forces the network to have a redundant representation\n",
    "* Prevents co-adaptation of features\n",
    "* Acts as model ensembles that share parameters\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66257993-0da77880-e7ca-11e9-8ced-d4f8450a1663.png\" width=\"500px\"></p>\n",
    "\n",
    "### ## Dropout - Backward pass\n",
    "\n",
    "* Only calculate gradient for neurons that was not dropped\n",
    "    * If dropped, then the gradient = 0\n",
    "    * Multiply by dropout mask for each layer\n",
    "\n",
    "### ## Dropout - Test time\n",
    "\n",
    "* Integrate out all the noise\n",
    "* Monte Carlo approximation\n",
    "    * Do many forward passes with different dropout masks on test images\n",
    "    * Average all predictions\n",
    "* Problem : not efficient\n",
    "* Solution : scale down at test time\n",
    "    * Multiply by dropout probability\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66258028-8c041a80-e7ca-11e9-8293-62270d62b8be.png\" width=\"500px\"></p>\n",
    "\n",
    "* At test time\n",
    "    * All neurons are active\n",
    "    * Must scale the activation so that each output neuron at test time = expected output during training time\n",
    "    * Changing the forward pass\n",
    "    \n",
    "### ## Inverted Dropout\n",
    "\n",
    "* Modify training time\n",
    "    * Scale up during the training time\n",
    "    * So the code at test time is unchanged\n",
    "    \n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/38347258/66258089-39772e00-e7cb-11e9-9193-acbff468bdf7.png\" width=\"500px\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Common Regularization\n",
    "\n",
    "---\n",
    "\n",
    "### ## Regularization: A Common Pattern\n",
    "\n",
    "* Training: \n",
    "    * Add some kind of randomness or noise\n",
    "    * Prevent model overfits training data \n",
    "\n",
    "* Testing: \n",
    "    * Average out or Approximate randomness\n",
    "    * (Hopefully) improve the generalization\n",
    "    \n",
    "### ## BatchNorm as a Regularization\n",
    "\n",
    "* Training: \n",
    "    * Normalize using stats from random minibatches\n",
    "\n",
    "* Testing: \n",
    "    * Use fixed stats to normalize\n",
    "\n",
    "Same regularization effect as dropout\n",
    "    * Use one at a time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
